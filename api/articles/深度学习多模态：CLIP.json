{"title":"深度学习多模态：CLIP","uid":"deafe07fcf5e4e17bdc29bc3bc203d8e","slug":"深度学习多模态：CLIP","date":"2022-11-06T03:54:00.000Z","updated":"2023-07-13T18:11:07.954Z","comments":true,"path":"api/articles/深度学习多模态：CLIP.json","keywords":"C++,Pyhton,Java,算法","cover":"/img/ai.jpeg","content":"<p>本篇章搬运自同校同学snowy的笔记。</p>\n<h4 id=\"什么是CLIP\"><a href=\"#什么是CLIP\" class=\"headerlink\" title=\"什么是CLIP\"></a>什么是CLIP</h4><p>Contrastive Language-Image Pre-Training—CLIP<br>利用文本的监督信号训练一个迁移能力强的视觉模型<br><img src=\"/img/CLIP/CLIP.jpg\" alt=\"1223.jpg\"></p>\n<ul>\n<li>这个模型有什么用呢？想象我们有一个图像分类的任务</li>\n<li>训练1000个类别，预测一张图片是这1000个类别中的哪一类</li>\n<li>现在如果加入50个新的类别的图像，试想会发生什么呢？</li>\n<li>传统的图像分类模型无法对类别进行拓展，想要保证准确率只能从头开始训练，费时费力。</li>\n<li>CLIP模型就可以用来解决这种问题，预训练后的模型就可以直接进行zero-shot</li>\n</ul>\n<p>与前人工作对比：</p>\n<ul>\n<li>CLIP论文指出，17年就已经开始有这些方法了，但是没获得太多关注。</li>\n<li>17年类似方法在ImageNet上的效果有17%。</li>\n<li>然后openAI说：不是方法不行，而是资源不到位（暴力出奇迹）</li>\n<li>一个648解决不了，那就再来十次648.。。。</li>\n</ul>\n<p>CLIP的成果：</p>\n<ul>\n<li>CLIP在完全不使用ImageNet中所有训练数据的前提下</li>\n<li>直接Zero-shot得到的结果与ResNet在128W ImageNet数据训练效果一致</li>\n<li>CLIP使用4亿个配对的数据和文本来进行训练，不标注直接爬取（没有解决transformer训练所需数据量大的缺点）</li>\n</ul>\n<h4 id=\"监督训练和zero-shot\"><a href=\"#监督训练和zero-shot\" class=\"headerlink\" title=\"监督训练和zero-shot\"></a>监督训练和zero-shot</h4><p>在监督学习中，计算机通过示例学习。它从过去的数据中学习，并将学习的结果应用到当前的数据中，以预测未来的事件。在这种情况下，输入和期望的输出数据都有助于预测未来事件。<br>无监督学习是训练机器使用既未分类也未标记的数据的方法。这意味着无法提供训练数据，机器只能自行学习。机器必须能够对数据进行分类，而无需事先提供任何有关数据的信息。<br>简而言之：</p>\n<ul>\n<li>有监督训练：利用已经打好标签的数据训练模型。</li>\n<li>无监督训练：训练所用的数据没有任何标签。</li>\n</ul>\n<p>什么是zero-shot（零样本学习）：</p>\n<ul>\n<li>定义 zero-shot顾名思义即是对某些类别完全不提供训练样本，也就是说没有标注样本的迁移任务被称为zero-shot。</li>\n<li>不需要任何训练样本就可以直接进行预测</li>\n<li>模仿人脑的学习能力和知识的迁移能力，根据以往的经验对未知的事物做出预测。</li>\n</ul>\n<p>简单的zero-shot的实例：<br><img src=\"/img/CLIP/CLIP2.jpg\" alt=\"123.jpg\"><br>首先，我们可以将其视为一个类似于自然语言处理的任务，它使用词嵌入（将词汇表中的词或短语映射到实数向量，要求具有相似含义的词将具有相似的词嵌入）。那么对于上面的例子，零样本学习是下面这样来处理的，</p>\n<ul>\n<li>训练数据中并没有斑马的图像，但是有带条纹的动物（如老虎），有跟马长得相似的一类动物（如马、驴），还有黑白色的动物（如熊猫）的各种图像。可以提取这些图像的特征（条纹、形状似马、黑/白色）并生成词嵌入，组成字典。</li>\n<li>然后，我们描述斑马的外观，并使用前面训练集里提出的特征来将斑马的外观转化成相应的词嵌入。</li>\n<li>最后，当你给模型输入一张斑马的图像，它会先提取图像的特征，转化成词嵌入，然后与字典中最接近的词嵌入进行比较，得出那图像可能是只斑马。</li>\n</ul>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span></blockquote>\n<h4 id=\"CLIP模型的基本架构\"><a href=\"#CLIP模型的基本架构\" class=\"headerlink\" title=\"CLIP模型的基本架构\"></a>CLIP模型的基本架构</h4><p>模型训练：<br><img src=\"/img/CLIP/CLIP.jpg\" alt=\"1223.jpg\"></p>\n<ul>\n<li>输入图片-&gt;图像编码器（vision transformer）-&gt;图片特征向量</li>\n<li>输入文字-&gt;文本编码器（text ）-&gt;文本特征向量</li>\n<li>对两个特征进行线性投射，得到相同维度的特征，并进行L2归一化</li>\n<li>计算两个特征向量的相似度（夹角余弦）</li>\n<li>对n个类别进行softmax，确定个正样本和个负样本，并最大化正样本的权重。</li>\n</ul>\n<div class=\"language-python\"><button title=\"Copy code\" class=\"copy\"></button><span class=\"lang\">python</span><pre class=\"shiki dark-plus\" style=\"background-color: #1a1a1a\" tabindex=\"0\"><code><span class=\"line\"><span style=\"color: #6A9955\"># 分别提取图像特征和文本特征</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">I_f = image_encoder(I) </span><span style=\"color: #6A9955\">#[n, d_i]</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">T_f = text_encoder(T) </span><span style=\"color: #6A9955\">#[n, d_t]</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color: #6A9955\"># 对两个特征进行线性投射，得到相同维度的特征，并进行l2归一化</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">I_e = l2_normalize(np.dot(I_f, W_i), </span><span style=\"color: #9CDCFE\">axis</span><span style=\"color: #D4D4D4\">=</span><span style=\"color: #B5CEA8\">1</span><span style=\"color: #D4D4D4\">)</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">T_e = l2_normalize(np.dot(T_f, W_t), </span><span style=\"color: #9CDCFE\">axis</span><span style=\"color: #D4D4D4\">=</span><span style=\"color: #B5CEA8\">1</span><span style=\"color: #D4D4D4\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color: #6A9955\"># 计算缩放的余弦相似度：[n, n]</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">logits = np.dot(I_e, T_e.T) * np.exp(t)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color: #6A9955\"># 对称的对比学习损失：等价于N个类别的cross_entropy_loss</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">labels = np.arange(n) </span><span style=\"color: #6A9955\"># 对角线元素的labels</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">loss_i = cross_entropy_loss(logits, labels, </span><span style=\"color: #9CDCFE\">axis</span><span style=\"color: #D4D4D4\">=</span><span style=\"color: #B5CEA8\">0</span><span style=\"color: #D4D4D4\">)</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">loss_t = cross_entropy_loss(logits, labels, </span><span style=\"color: #9CDCFE\">axis</span><span style=\"color: #D4D4D4\">=</span><span style=\"color: #B5CEA8\">1</span><span style=\"color: #D4D4D4\">)</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">loss = (loss_i + loss_t)/</span><span style=\"color: #B5CEA8\">2</span></span></code></pre></div><p>模型预测：<br><img src=\"/img/CLIP/CLIP3.png\" alt=\"image.png\"></p>\n<ul>\n<li>给出一些文本提升（给出选项）</li>\n<li>选项中要包含正确答案</li>\n<li>然后计算每一个文本提升和图片特征的相似度。</li>\n<li>找到相似度最高的即为正确答案</li>\n</ul>\n<p>合理的提示：</p>\n<ul>\n<li>预测时的提示非常重要</li>\n<li>首先是需要一句话或者几个词来提示</li>\n<li>最好要加上预测的场景，要具有情景的相关性</li>\n<li>提示要全面，这样预测准确率也会提高。</li>\n</ul>\n<blockquote></blockquote>\n<h4 id=\"CLIP模型的展示\"><a href=\"#CLIP模型的展示\" class=\"headerlink\" title=\"CLIP模型的展示\"></a>CLIP模型的展示</h4>","feature":true,"text":"本篇章搬运自同校同学snowy的笔记。 什么是CLIPContrastive Language-Image Pre-Training—CLIP利用文本的监督信号...","permalink":"/post/深度学习多模态：CLIP","photos":[],"count_time":{"symbolsCount":"2.1k","symbolsTime":"2 mins."},"categories":[{"name":"人工智能","slug":"人工智能","count":4,"path":"api/categories/人工智能.json"}],"tags":[{"name":"Transformer","slug":"Transformer","count":1,"path":"api/tags/Transformer.json"},{"name":"多模态学习","slug":"多模态学习","count":1,"path":"api/tags/多模态学习.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E4%BB%80%E4%B9%88%E6%98%AFCLIP\"><span class=\"toc-text\">什么是CLIP</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E7%9B%91%E7%9D%A3%E8%AE%AD%E7%BB%83%E5%92%8Czero-shot\"><span class=\"toc-text\">监督训练和zero-shot</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#CLIP%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%9E%B6%E6%9E%84\"><span class=\"toc-text\">CLIP模型的基本架构</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#CLIP%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%B1%95%E7%A4%BA\"><span class=\"toc-text\">CLIP模型的展示</span></a></li></ol>","author":{"name":"Ac-Accelerator","slug":"blog-author","avatar":"/img/avatar.jpg","link":"/","description":"一位计算机技术爱好者，大学在读，目前正在学习C++、Python、Java和计算机系统以及计算机视觉深度学习等相关知识，欢迎交流","socials":{"github":"https://github.com/Ac-Accelerator","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"https://blog.csdn.net/qq_31702515?spm=1000.2115.3001.5343","juejin":"","customs":{"qq":{"icon":"/svg/QQ.svg","link":"http://tool.gljlw.com/qq/?qq=2583832841"}}}},"mapped":true,"hidden":false,"prev_post":{"title":"C++ RAII与智能指针","uid":"182933b43edb5a2e2aab1a699c004ce0","slug":"C-RAII与智能指针","date":"2022-11-06T04:22:00.000Z","updated":"2023-09-25T16:37:57.933Z","comments":true,"path":"api/articles/C-RAII与智能指针.json","keywords":"C++,Pyhton,Java,算法","cover":"/img/CPPcover.webp","text":"这是观看一位大佬双笙子佯谬的视频后写的笔记。 RAII（Resource Acquisition Is Initialization）资源获取视为初始化，反之，...","permalink":"/post/C-RAII与智能指针","photos":[],"count_time":{"symbolsCount":"9.1k","symbolsTime":"8 mins."},"categories":[{"name":"C++","slug":"C","count":6,"path":"api/categories/C.json"}],"tags":[{"name":"内存安全","slug":"内存安全","count":1,"path":"api/tags/内存安全.json"},{"name":"C++进阶","slug":"C-进阶","count":4,"path":"api/tags/C-进阶.json"}],"author":{"name":"Ac-Accelerator","slug":"blog-author","avatar":"/img/avatar.jpg","link":"/","description":"一位计算机技术爱好者，大学在读，目前正在学习C++、Python、Java和计算机系统以及计算机视觉深度学习等相关知识，欢迎交流","socials":{"github":"https://github.com/Ac-Accelerator","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"https://blog.csdn.net/qq_31702515?spm=1000.2115.3001.5343","juejin":"","customs":{"qq":{"icon":"/svg/QQ.svg","link":"http://tool.gljlw.com/qq/?qq=2583832841"}}}},"feature":true},"next_post":{"title":"强化学习","uid":"4d1e3a4a4afff0b0d31e07df9ad79915","slug":"强化学习","date":"2022-07-14T13:23:00.000Z","updated":"2023-09-25T17:25:04.084Z","comments":true,"path":"api/articles/强化学习.json","keywords":"C++,Pyhton,Java,算法","cover":"/img/ai2.jpeg","text":"回顾监督学习： 学习一个模型，使得其能正确的将数据X映射到标签Y无监督学习 学习数据中潜在的结构以及信息 强化学习的概念强化学习主要由两个对象组成：Enviro...","permalink":"/post/强化学习","photos":[],"count_time":{"symbolsCount":"2.7k","symbolsTime":"2 mins."},"categories":[{"name":"人工智能","slug":"人工智能","count":4,"path":"api/categories/人工智能.json"}],"tags":[{"name":"强化学习","slug":"强化学习","count":1,"path":"api/tags/强化学习.json"}],"author":{"name":"Ac-Accelerator","slug":"blog-author","avatar":"/img/avatar.jpg","link":"/","description":"一位计算机技术爱好者，大学在读，目前正在学习C++、Python、Java和计算机系统以及计算机视觉深度学习等相关知识，欢迎交流","socials":{"github":"https://github.com/Ac-Accelerator","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"https://blog.csdn.net/qq_31702515?spm=1000.2115.3001.5343","juejin":"","customs":{"qq":{"icon":"/svg/QQ.svg","link":"http://tool.gljlw.com/qq/?qq=2583832841"}}}},"feature":true}}