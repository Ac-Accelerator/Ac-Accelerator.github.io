{"title":"深度学习的模型训练","uid":"2d84197b69c87d393530863c6fed2f9f","slug":"人工智能学习笔记","date":"2022-04-04T02:28:00.000Z","updated":"2023-03-31T12:31:25.149Z","comments":true,"path":"api/articles/人工智能学习笔记.json","keywords":"C++,Pyhton,Java,算法","cover":"/img/ai.jpeg","content":"<p>兄弟们，看了二十来天的人工智能概述还有实现方法，我终于搞懂了。</p>\n<h2 id=\"一个有关人工智能的pytorch程序应该包含\"><a href=\"#一个有关人工智能的pytorch程序应该包含\" class=\"headerlink\" title=\"一个有关人工智能的pytorch程序应该包含\"></a>一个有关人工智能的pytorch程序应该包含</h2><h3 id=\"数据集\"><a href=\"#数据集\" class=\"headerlink\" title=\"数据集\"></a>数据集</h3><p>废话，没数据怎么玩啊，这里以FashionMNIST数据集举个例子。</p>\n<div class=\"language-python\"><button title=\"Copy code\" class=\"copy\"></button><span class=\"lang\">python</span><pre class=\"shiki dark-plus\" style=\"background-color: #1a1a1a\" tabindex=\"0\"><code><span class=\"line\"><span style=\"color: #C586C0\">import</span><span style=\"color: #D4D4D4\"> torch</span></span>\n<span class=\"line\"><span style=\"color: #C586C0\">from</span><span style=\"color: #D4D4D4\"> torch </span><span style=\"color: #C586C0\">import</span><span style=\"color: #D4D4D4\"> nn</span></span>\n<span class=\"line\"><span style=\"color: #C586C0\">import</span><span style=\"color: #D4D4D4\"> torchvision</span></span>\n<span class=\"line\"><span style=\"color: #C586C0\">import</span><span style=\"color: #D4D4D4\"> torchvision.transforms </span><span style=\"color: #C586C0\">as</span><span style=\"color: #D4D4D4\"> transforms</span></span>\n<span class=\"line\"><span style=\"color: #6A9955\">#首先是数据集的内容。</span></span>\n<span class=\"line\"><span style=\"color: #6A9955\">#mnist_train代表要进行训练的数据集，root属性是其存放的位置，train属性代表这是用作训练的数据集，download属性为true表示无则下载，有则现用，transform属性代表将数据集转化为Tensor。</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">mnist_train = torchvision.datasets.FashionMNIST(</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">    </span><span style=\"color: #9CDCFE\">root</span><span style=\"color: #D4D4D4\">=</span><span style=\"color: #CE9178\">'./Datasets'</span><span style=\"color: #D4D4D4\">, </span><span style=\"color: #9CDCFE\">train</span><span style=\"color: #D4D4D4\">=</span><span style=\"color: #569CD6\">True</span><span style=\"color: #D4D4D4\">, </span><span style=\"color: #9CDCFE\">download</span><span style=\"color: #D4D4D4\">=</span><span style=\"color: #569CD6\">True</span><span style=\"color: #D4D4D4\">, </span><span style=\"color: #9CDCFE\">transform</span><span style=\"color: #D4D4D4\">=transforms.ToTensor())</span></span>\n<span class=\"line\"><span style=\"color: #6A9955\">#mnist_train代表是对训练结果进行测试的数据集，属性意义和上面的几乎一致</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">mnist_test = torchvision.datasets.FashionMNIST(</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">    </span><span style=\"color: #9CDCFE\">root</span><span style=\"color: #D4D4D4\">=</span><span style=\"color: #CE9178\">'./Datasets'</span><span style=\"color: #D4D4D4\">, </span><span style=\"color: #9CDCFE\">train</span><span style=\"color: #D4D4D4\">=</span><span style=\"color: #569CD6\">False</span><span style=\"color: #D4D4D4\">, </span><span style=\"color: #9CDCFE\">download</span><span style=\"color: #D4D4D4\">=</span><span style=\"color: #569CD6\">True</span><span style=\"color: #D4D4D4\">, </span><span style=\"color: #9CDCFE\">transform</span><span style=\"color: #D4D4D4\">=transforms.ToTensor())</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color: #6A9955\">#设置批大小，即迭代数据的时候每一批次样本的量</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">batch_size=</span><span style=\"color: #B5CEA8\">256</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color: #6A9955\">#train_iter代表对训练数据集迭代的迭代器，shuffle代表随机打乱顺序，这样符合机器学习的原理，num_workers为载入数据时使用处理器的数量，win下一般为0。</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">train_iter = torch.utils.data.DataLoader(</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">    mnist_train, </span><span style=\"color: #9CDCFE\">batch_size</span><span style=\"color: #D4D4D4\">=batch_size, </span><span style=\"color: #9CDCFE\">shuffle</span><span style=\"color: #D4D4D4\">=</span><span style=\"color: #569CD6\">True</span><span style=\"color: #D4D4D4\">, </span><span style=\"color: #9CDCFE\">num_workers</span><span style=\"color: #D4D4D4\">=</span><span style=\"color: #B5CEA8\">0</span><span style=\"color: #D4D4D4\">)</span></span>\n<span class=\"line\"><span style=\"color: #6A9955\">#test_iter的shuffle为false不必打乱。</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">test_iter = torch.utils.data.DataLoader(</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">    mnist_test, </span><span style=\"color: #9CDCFE\">batch_size</span><span style=\"color: #D4D4D4\">=batch_size, </span><span style=\"color: #9CDCFE\">shuffle</span><span style=\"color: #D4D4D4\">=</span><span style=\"color: #569CD6\">False</span><span style=\"color: #D4D4D4\">, </span><span style=\"color: #9CDCFE\">num_workers</span><span style=\"color: #D4D4D4\">=</span><span style=\"color: #B5CEA8\">0</span><span style=\"color: #D4D4D4\">)</span></span></code></pre></div><h3 id=\"模型\"><a href=\"#模型\" class=\"headerlink\" title=\"模型\"></a>模型</h3><p>有了数据，那我们该采取哪种方式处理呢？那么就需要模型了。</p>\n<h4 id=\"自定义不带参数的层：\"><a href=\"#自定义不带参数的层：\" class=\"headerlink\" title=\"自定义不带参数的层：\"></a>自定义不带参数的层：</h4><div class=\"language-python\"><button title=\"Copy code\" class=\"copy\"></button><span class=\"lang\">python</span><pre class=\"shiki dark-plus\" style=\"background-color: #1a1a1a\" tabindex=\"0\"><code><span class=\"line\"><span style=\"color: #6A9955\">#CenteredLayer类通过继承Module类自定义了一个将输入减掉均值后输出的层，并将层的计算定义在了forward函数里。这个层不含模型参数。</span></span>\n<span class=\"line\"><span style=\"color: #569CD6\">class</span><span style=\"color: #D4D4D4\"> </span><span style=\"color: #4EC9B0\">CenteredLayer</span><span style=\"color: #D4D4D4\">(</span><span style=\"color: #4EC9B0\">nn</span><span style=\"color: #D4D4D4\">.</span><span style=\"color: #4EC9B0\">Module</span><span style=\"color: #D4D4D4\">):</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">    </span><span style=\"color: #569CD6\">def</span><span style=\"color: #D4D4D4\"> </span><span style=\"color: #DCDCAA\">__init__</span><span style=\"color: #D4D4D4\">(</span><span style=\"color: #9CDCFE\">self</span><span style=\"color: #D4D4D4\">, **</span><span style=\"color: #9CDCFE\">kwargs</span><span style=\"color: #D4D4D4\">):</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">        </span><span style=\"color: #4EC9B0\">super</span><span style=\"color: #D4D4D4\">(CenteredLayer, </span><span style=\"color: #569CD6\">self</span><span style=\"color: #D4D4D4\">).</span><span style=\"color: #DCDCAA\">__init__</span><span style=\"color: #D4D4D4\">(**kwargs)</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">    </span><span style=\"color: #569CD6\">def</span><span style=\"color: #D4D4D4\"> </span><span style=\"color: #DCDCAA\">forward</span><span style=\"color: #D4D4D4\">(</span><span style=\"color: #9CDCFE\">self</span><span style=\"color: #D4D4D4\">, </span><span style=\"color: #9CDCFE\">x</span><span style=\"color: #D4D4D4\">):</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">        </span><span style=\"color: #C586C0\">return</span><span style=\"color: #D4D4D4\"> x - x.mean()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color: #6A9955\">#FlattenLayer类通过继承Module类自定义了一个将输入X的维度降低为2维，第一维是数据的量，第二维是数据的内容</span></span>\n<span class=\"line\"><span style=\"color: #569CD6\">class</span><span style=\"color: #D4D4D4\"> </span><span style=\"color: #4EC9B0\">FlattenLayer</span><span style=\"color: #D4D4D4\">(</span><span style=\"color: #4EC9B0\">nn</span><span style=\"color: #D4D4D4\">.</span><span style=\"color: #4EC9B0\">Module</span><span style=\"color: #D4D4D4\">):</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">    </span><span style=\"color: #569CD6\">def</span><span style=\"color: #D4D4D4\"> </span><span style=\"color: #DCDCAA\">__init__</span><span style=\"color: #D4D4D4\">(</span><span style=\"color: #9CDCFE\">self</span><span style=\"color: #D4D4D4\">):</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">        </span><span style=\"color: #4EC9B0\">super</span><span style=\"color: #D4D4D4\">(FlattenLayer, </span><span style=\"color: #569CD6\">self</span><span style=\"color: #D4D4D4\">).</span><span style=\"color: #DCDCAA\">__init__</span><span style=\"color: #D4D4D4\">()</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">    </span><span style=\"color: #569CD6\">def</span><span style=\"color: #D4D4D4\"> </span><span style=\"color: #DCDCAA\">forward</span><span style=\"color: #D4D4D4\">(</span><span style=\"color: #9CDCFE\">self</span><span style=\"color: #D4D4D4\">, </span><span style=\"color: #9CDCFE\">x</span><span style=\"color: #D4D4D4\">):  </span><span style=\"color: #6A9955\"># x shape: (batch, *, *, ...)</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">        </span><span style=\"color: #C586C0\">return</span><span style=\"color: #D4D4D4\"> x.view(x.shape[</span><span style=\"color: #B5CEA8\">0</span><span style=\"color: #D4D4D4\">], -</span><span style=\"color: #B5CEA8\">1</span><span style=\"color: #D4D4D4\">)</span></span></code></pre></div><h4 id=\"自定义带参数的层：\"><a href=\"#自定义带参数的层：\" class=\"headerlink\" title=\"自定义带参数的层：\"></a>自定义带参数的层：</h4><div class=\"language-python\"><button title=\"Copy code\" class=\"copy\"></button><span class=\"lang\">python</span><pre class=\"shiki dark-plus\" style=\"background-color: #1a1a1a\" tabindex=\"0\"><code><span class=\"line\"><span style=\"color: #6A9955\">#这里的例子是定义了一个按列表顺序向前传播的层</span></span>\n<span class=\"line\"><span style=\"color: #569CD6\">class</span><span style=\"color: #D4D4D4\"> </span><span style=\"color: #4EC9B0\">MyDense</span><span style=\"color: #D4D4D4\">(</span><span style=\"color: #4EC9B0\">nn</span><span style=\"color: #D4D4D4\">.</span><span style=\"color: #4EC9B0\">Module</span><span style=\"color: #D4D4D4\">):</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">    </span><span style=\"color: #569CD6\">def</span><span style=\"color: #D4D4D4\"> </span><span style=\"color: #DCDCAA\">__init__</span><span style=\"color: #D4D4D4\">(</span><span style=\"color: #9CDCFE\">self</span><span style=\"color: #D4D4D4\">):</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">        </span><span style=\"color: #4EC9B0\">super</span><span style=\"color: #D4D4D4\">(MyDense, </span><span style=\"color: #569CD6\">self</span><span style=\"color: #D4D4D4\">).</span><span style=\"color: #DCDCAA\">__init__</span><span style=\"color: #D4D4D4\">()</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">        </span><span style=\"color: #569CD6\">self</span><span style=\"color: #D4D4D4\">.params = nn.ParameterList([nn.Parameter(torch.randn(</span><span style=\"color: #B5CEA8\">4</span><span style=\"color: #D4D4D4\">, </span><span style=\"color: #B5CEA8\">4</span><span style=\"color: #D4D4D4\">)) </span><span style=\"color: #C586C0\">for</span><span style=\"color: #D4D4D4\"> i </span><span style=\"color: #C586C0\">in</span><span style=\"color: #D4D4D4\"> </span><span style=\"color: #DCDCAA\">range</span><span style=\"color: #D4D4D4\">(</span><span style=\"color: #B5CEA8\">3</span><span style=\"color: #D4D4D4\">)])</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">        </span><span style=\"color: #569CD6\">self</span><span style=\"color: #D4D4D4\">.params.append(nn.Parameter(torch.randn(</span><span style=\"color: #B5CEA8\">4</span><span style=\"color: #D4D4D4\">, </span><span style=\"color: #B5CEA8\">1</span><span style=\"color: #D4D4D4\">)))</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">    </span><span style=\"color: #569CD6\">def</span><span style=\"color: #D4D4D4\"> </span><span style=\"color: #DCDCAA\">forward</span><span style=\"color: #D4D4D4\">(</span><span style=\"color: #9CDCFE\">self</span><span style=\"color: #D4D4D4\">, </span><span style=\"color: #9CDCFE\">x</span><span style=\"color: #D4D4D4\">):</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">        </span><span style=\"color: #C586C0\">for</span><span style=\"color: #D4D4D4\"> i </span><span style=\"color: #C586C0\">in</span><span style=\"color: #D4D4D4\"> </span><span style=\"color: #DCDCAA\">range</span><span style=\"color: #D4D4D4\">(</span><span style=\"color: #DCDCAA\">len</span><span style=\"color: #D4D4D4\">(</span><span style=\"color: #569CD6\">self</span><span style=\"color: #D4D4D4\">.params)):</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">            x = torch.mm(x, </span><span style=\"color: #569CD6\">self</span><span style=\"color: #D4D4D4\">.params[i])</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">        </span><span style=\"color: #C586C0\">return</span><span style=\"color: #D4D4D4\"> x</span></span></code></pre></div><p>这是自定义的模型样本，我来解释一下。</p>\n<h4 id=\"定义模型并生成模型实例\"><a href=\"#定义模型并生成模型实例\" class=\"headerlink\" title=\"定义模型并生成模型实例\"></a>定义模型并生成模型实例</h4><div class=\"language-python\"><button title=\"Copy code\" class=\"copy\"></button><span class=\"lang\">python</span><pre class=\"shiki dark-plus\" style=\"background-color: #1a1a1a\" tabindex=\"0\"><code><span class=\"line\"><span style=\"color: #6A9955\">#一般情况下自定义的模型都以类的形式声明，继承自nn.Module类</span></span>\n<span class=\"line\"><span style=\"color: #6A9955\">#这里是一个自定义的多层感知机</span></span>\n<span class=\"line\"><span style=\"color: #569CD6\">class</span><span style=\"color: #D4D4D4\"> </span><span style=\"color: #4EC9B0\">MLP</span><span style=\"color: #D4D4D4\">(</span><span style=\"color: #4EC9B0\">nn</span><span style=\"color: #D4D4D4\">.</span><span style=\"color: #4EC9B0\">Module</span><span style=\"color: #D4D4D4\">):</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">    </span><span style=\"color: #569CD6\">def</span><span style=\"color: #D4D4D4\"> </span><span style=\"color: #DCDCAA\">__init__</span><span style=\"color: #D4D4D4\">(</span><span style=\"color: #9CDCFE\">self</span><span style=\"color: #D4D4D4\">):</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">        </span><span style=\"color: #4EC9B0\">super</span><span style=\"color: #D4D4D4\">(MLP, </span><span style=\"color: #569CD6\">self</span><span style=\"color: #D4D4D4\">).</span><span style=\"color: #DCDCAA\">__init__</span><span style=\"color: #D4D4D4\">()</span><span style=\"color: #6A9955\"># 调用MLP父类Module的构造函数来进行必要的初始化。</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">        </span><span style=\"color: #569CD6\">self</span><span style=\"color: #D4D4D4\">.flat=FlattenLayer()</span><span style=\"color: #6A9955\">#自定义的层</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">        </span><span style=\"color: #569CD6\">self</span><span style=\"color: #D4D4D4\">.hidden1 = nn.Linear(</span><span style=\"color: #B5CEA8\">784</span><span style=\"color: #D4D4D4\">, </span><span style=\"color: #B5CEA8\">512</span><span style=\"color: #D4D4D4\">) </span><span style=\"color: #6A9955\"># 全连接隐藏层1</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">        </span><span style=\"color: #569CD6\">self</span><span style=\"color: #D4D4D4\">.hidden2 = nn.Linear(</span><span style=\"color: #B5CEA8\">512</span><span style=\"color: #D4D4D4\">, </span><span style=\"color: #B5CEA8\">256</span><span style=\"color: #D4D4D4\">)</span><span style=\"color: #6A9955\"># 全连接隐藏层2</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">        </span><span style=\"color: #569CD6\">self</span><span style=\"color: #D4D4D4\">.output = nn.Linear(</span><span style=\"color: #B5CEA8\">256</span><span style=\"color: #D4D4D4\">, </span><span style=\"color: #B5CEA8\">10</span><span style=\"color: #D4D4D4\">)  </span><span style=\"color: #6A9955\"># 全连接输出层</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">        </span><span style=\"color: #569CD6\">self</span><span style=\"color: #D4D4D4\">.act = nn.ReLU() </span><span style=\"color: #6A9955\">#激活函数</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">        </span><span style=\"color: #6A9955\">#注意，在使用Linear或其他相关函数定义层的时候，就已经了自带的权重参数。</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">    </span><span style=\"color: #6A9955\"># 需要定义模型的向前传播算法，即如何根据输入x计算返回所需要的模型输出。</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">    </span><span style=\"color: #569CD6\">def</span><span style=\"color: #D4D4D4\"> </span><span style=\"color: #DCDCAA\">forward</span><span style=\"color: #D4D4D4\">(</span><span style=\"color: #9CDCFE\">self</span><span style=\"color: #D4D4D4\">, </span><span style=\"color: #9CDCFE\">x</span><span style=\"color: #D4D4D4\">):</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">        </span><span style=\"color: #6A9955\">#在调用的时候将以这种方式向前传播</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">        </span><span style=\"color: #C586C0\">return</span><span style=\"color: #D4D4D4\"> </span><span style=\"color: #569CD6\">self</span><span style=\"color: #D4D4D4\">.output(</span><span style=\"color: #569CD6\">self</span><span style=\"color: #D4D4D4\">.act(</span><span style=\"color: #569CD6\">self</span><span style=\"color: #D4D4D4\">.hidden2(</span><span style=\"color: #569CD6\">self</span><span style=\"color: #D4D4D4\">.act(</span><span style=\"color: #569CD6\">self</span><span style=\"color: #D4D4D4\">.hidden1(</span><span style=\"color: #569CD6\">self</span><span style=\"color: #D4D4D4\">.flat(x))))))</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">net=MLP()\t</span><span style=\"color: #6A9955\">#生成对象实例</span></span></code></pre></div><p>使用Module类自带的子类Sequential类可以更加方便的进行模型构造，但是不能构造复杂的模型。类似于这样的使用</p>\n<div class=\"language-python\"><button title=\"Copy code\" class=\"copy\"></button><span class=\"lang\">python</span><pre class=\"shiki dark-plus\" style=\"background-color: #1a1a1a\" tabindex=\"0\"><code><span class=\"line\"><span style=\"color: #6A9955\">#比如这个例子，它的向前传播算法是这样的</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">net = nn.Sequential(</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">    nn.Linear(num_inputs, num_hiddens),</span><span style=\"color: #6A9955\">#第一步，计算隐藏层1</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">    nn.ReLU(),</span><span style=\"color: #6A9955\">#对隐藏层1的结果使用激活函数</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">    nn.Dropout(</span><span style=\"color: #B5CEA8\">0.1</span><span style=\"color: #D4D4D4\">),</span><span style=\"color: #6A9955\">#对隐藏层的结果使用丢弃法，丢弃概率为0.1</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">    nn.Linear(num_hiddens,</span><span style=\"color: #B5CEA8\">256</span><span style=\"color: #D4D4D4\">),</span><span style=\"color: #6A9955\">#计算隐藏层2</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">    nn.ReLU(),</span><span style=\"color: #6A9955\">#对隐藏层2的结果使用激活函数</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">    nn.Dropout(</span><span style=\"color: #B5CEA8\">0.2</span><span style=\"color: #D4D4D4\">),</span><span style=\"color: #6A9955\">#对隐藏层的结果使用丢弃法，丢弃概率为0.2</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">    nn.Linear(</span><span style=\"color: #B5CEA8\">256</span><span style=\"color: #D4D4D4\">, num_outputs),</span><span style=\"color: #6A9955\">#计算输出层</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">)</span></span></code></pre></div><p>可以看到Sequential类的特点就是按顺序进行向前传播。非常直观。Sequential内的模块需要按照顺序排列，要保证相邻层的输入输出大小相匹配，内部forward功能已经实现。<br>其他的还有ModuleList和ModuleDict，但是它们的forward方法需要自己去定义，个人觉得一般般（任何事物都有其存在的价值，可能是我才疏学浅吧）。</p>\n<h4 id=\"访问模型中的参数：\"><a href=\"#访问模型中的参数：\" class=\"headerlink\" title=\"访问模型中的参数：\"></a>访问模型中的参数：</h4><div class=\"language-python\"><button title=\"Copy code\" class=\"copy\"></button><span class=\"lang\">python</span><pre class=\"shiki dark-plus\" style=\"background-color: #1a1a1a\" tabindex=\"0\"><code><span class=\"line\"><span style=\"color: #6A9955\">#net是一个实例化的模型对象，使用named_parameters可以访问&lt;名称，参数&gt;键值对。</span></span>\n<span class=\"line\"><span style=\"color: #6A9955\">#name是个str,param是一个Parameter对象。</span></span>\n<span class=\"line\"><span style=\"color: #C586C0\">for</span><span style=\"color: #D4D4D4\"> name, param </span><span style=\"color: #C586C0\">in</span><span style=\"color: #D4D4D4\"> net.named_parameters():</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">    </span><span style=\"color: #6A9955\">#在这里什么也不干，单纯就打印出来</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">    </span><span style=\"color: #DCDCAA\">print</span><span style=\"color: #D4D4D4\">(name, param.size())</span></span></code></pre></div><h4 id=\"初始化模型中的参数：\"><a href=\"#初始化模型中的参数：\" class=\"headerlink\" title=\"初始化模型中的参数：\"></a>初始化模型中的参数：</h4><div class=\"language-python\"><button title=\"Copy code\" class=\"copy\"></button><span class=\"lang\">python</span><pre class=\"shiki dark-plus\" style=\"background-color: #1a1a1a\" tabindex=\"0\"><code><span class=\"line\"><span style=\"color: #6A9955\">#nn.Module的模块参数都采取了较为合理的初始化策略，但是我们有时候依然希望自定义模型中的参数</span></span>\n<span class=\"line\"><span style=\"color: #6A9955\">#使用init方法对权重进行初始化，除了normal_外还有其他的初始化函数。</span></span>\n<span class=\"line\"><span style=\"color: #C586C0\">for</span><span style=\"color: #D4D4D4\"> name, param </span><span style=\"color: #C586C0\">in</span><span style=\"color: #D4D4D4\"> net.named_parameters():</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">    </span><span style=\"color: #C586C0\">if</span><span style=\"color: #D4D4D4\"> </span><span style=\"color: #CE9178\">'weight'</span><span style=\"color: #D4D4D4\"> </span><span style=\"color: #569CD6\">in</span><span style=\"color: #D4D4D4\"> name:</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">        init.normal_(param, </span><span style=\"color: #9CDCFE\">mean</span><span style=\"color: #D4D4D4\">=</span><span style=\"color: #B5CEA8\">0</span><span style=\"color: #D4D4D4\">, </span><span style=\"color: #9CDCFE\">std</span><span style=\"color: #D4D4D4\">=</span><span style=\"color: #B5CEA8\">0.01</span><span style=\"color: #D4D4D4\">)</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">        </span><span style=\"color: #DCDCAA\">print</span><span style=\"color: #D4D4D4\">(name, param.data)</span></span></code></pre></div><p>模型参数可以共用，只要向前传播的时候调用的是同一个模型参数即可。</p>\n<h4 id=\"读写和保存\"><a href=\"#读写和保存\" class=\"headerlink\" title=\"读写和保存\"></a>读写和保存</h4><p>对Tensor：</p>\n<div class=\"language-python\"><button title=\"Copy code\" class=\"copy\"></button><span class=\"lang\">python</span><pre class=\"shiki dark-plus\" style=\"background-color: #1a1a1a\" tabindex=\"0\"><code><span class=\"line\"><span style=\"color: #D4D4D4\">x = torch.ones(</span><span style=\"color: #B5CEA8\">3</span><span style=\"color: #D4D4D4\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color: #6A9955\">#将tensor x 保存到x.pt文件里面</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">torch.save(x, </span><span style=\"color: #CE9178\">'x.pt'</span><span style=\"color: #D4D4D4\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color: #6A9955\">#将tensor x 从x.pt文件里面读取到x2</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">x2 = torch.load(</span><span style=\"color: #CE9178\">'x.pt'</span><span style=\"color: #D4D4D4\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color: #6A9955\">#还可以将x,x2形成列表储存进xx2.pt，</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">torch.save([x, x2], </span><span style=\"color: #CE9178\">'xx2.pt'</span><span style=\"color: #D4D4D4\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color: #6A9955\">#再读取到xx2_list里</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">xx2_list = torch.load(</span><span style=\"color: #CE9178\">'xx2.pt'</span><span style=\"color: #D4D4D4\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color: #6A9955\">#字典同理</span></span></code></pre></div><p>对模型呢：</p>\n<div class=\"language-python\"><button title=\"Copy code\" class=\"copy\"></button><span class=\"lang\">python</span><pre class=\"shiki dark-plus\" style=\"background-color: #1a1a1a\" tabindex=\"0\"><code><span class=\"line\"><span style=\"color: #6A9955\">#这里使用之前定义的MLP模型来举例</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color: #6A9955\">#只保存和加载模型参数的方法</span></span>\n<span class=\"line\"><span style=\"color: #6A9955\"># 注意，使用state_dict方法时一定要先有一个实例化的对象</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">model=MLP()</span></span>\n<span class=\"line\"><span style=\"color: #6A9955\">#使用state_dict()方法保存和加载模型参数，这里的model是一个实例化的模型对象，PATH是要保存的路径还有文件名</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">torch.save(model.state_dict(), PATH) </span><span style=\"color: #6A9955\"># 推荐的文件后缀名是pt或pth</span></span>\n<span class=\"line\"><span style=\"color: #6A9955\">#在载入的时候也要有一个实例对象，这里就命名为model2吧</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">model2=MLP()</span></span>\n<span class=\"line\"><span style=\"color: #6A9955\">#载入！</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">model.load_state_dict(torch.load(PATH))</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color: #6A9955\">#保存和加载整个模型的方法</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">torch.save(model, PATH)</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">model = torch.load(PATH)</span></span></code></pre></div><h3 id=\"损失函数\"><a href=\"#损失函数\" class=\"headerlink\" title=\"损失函数\"></a>损失函数</h3><p>可以自己写，也可以使用自带的。<br>例如使用自带的CrossEntropyLoss()交叉熵损失函数。</p>\n<div class=\"language-python\"><button title=\"Copy code\" class=\"copy\"></button><span class=\"lang\">python</span><pre class=\"shiki dark-plus\" style=\"background-color: #1a1a1a\" tabindex=\"0\"><code><span class=\"line\"><span style=\"color: #D4D4D4\">loss = torch.nn.CrossEntropyLoss()</span></span></code></pre></div><p>其他的还有</p>\n<div class=\"language-python\"><button title=\"Copy code\" class=\"copy\"></button><span class=\"lang\">python</span><pre class=\"shiki dark-plus\" style=\"background-color: #1a1a1a\" tabindex=\"0\"><code><span class=\"line\"><span style=\"color: #D4D4D4\">torch.nn.L1Loss()</span><span style=\"color: #6A9955\">#绝对值损失</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">torch.nn.MSELoss()</span><span style=\"color: #6A9955\">#均方差损失</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">torch.nn.KLDivLoss()</span><span style=\"color: #6A9955\">#散度损失</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">torch.nn.BCELos()</span><span style=\"color: #6A9955\">#二进制交叉熵损失</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">torch.nn.BCEWithLogitsLoss()</span><span style=\"color: #6A9955\"># 带Sigmoid层的二进制交叉熵损失</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">torch.nn.MarginRankingLoss()</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">torch.nn.HingeEmbeddingLoss()</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">torch.nn.MultiLabelMarginLoss</span><span style=\"color: #6A9955\">#多标签分类损失</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">torch.nn.SmoothL1Loss()</span><span style=\"color: #6A9955\">#平滑版绝对值损失</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">torch.nn.SoftMarginLoss()</span><span style=\"color: #6A9955\">#2分类的logistic损失</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">torch.nn.MultiLabelSoftMarginLoss()</span><span style=\"color: #6A9955\">#多标签 one-versus-all 损失</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">torch.nn.CosineEmbeddingLoss()</span><span style=\"color: #6A9955\">#cosine损失</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">torch.nn.MultiMarginLoss()</span><span style=\"color: #6A9955\">#多类别分类的hinge损失</span></span></code></pre></div><p>参见<a href=\"https://3water.com/article/eOTcy4NjIu\">博客</a></p>\n<h3 id=\"优化函数\"><a href=\"#优化函数\" class=\"headerlink\" title=\"优化函数\"></a>优化函数</h3><p>可以自己写，也可以使用自带的。<br>例如使用自带的SGD函数。</p>\n<div class=\"language-python\"><button title=\"Copy code\" class=\"copy\"></button><span class=\"lang\">python</span><pre class=\"shiki dark-plus\" style=\"background-color: #1a1a1a\" tabindex=\"0\"><code><span class=\"line\"><span style=\"color: #6A9955\">#这里可以设置，lr学习率，weight_decay权重衰减</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">optimizer = torch.optim.SGD(net.parameters(), </span><span style=\"color: #9CDCFE\">lr</span><span style=\"color: #D4D4D4\">=</span><span style=\"color: #B5CEA8\">0.4</span><span style=\"color: #D4D4D4\">,</span><span style=\"color: #9CDCFE\">weight_decay</span><span style=\"color: #D4D4D4\">=</span><span style=\"color: #B5CEA8\">0.0005</span><span style=\"color: #D4D4D4\">)</span></span></code></pre></div><p>其他的还有</p>\n<div class=\"language-python\"><button title=\"Copy code\" class=\"copy\"></button><span class=\"lang\">python</span><pre class=\"shiki dark-plus\" style=\"background-color: #1a1a1a\" tabindex=\"0\"><code><span class=\"line\"><span style=\"color: #D4D4D4\">torch.optim.SGD()</span><span style=\"color: #6A9955\">#随机梯度下降算法</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">torch.optim.ASGD()</span><span style=\"color: #6A9955\">#异步随机梯度下降法</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">torch.optim.Rprop()</span><span style=\"color: #6A9955\">#弹性反向传播算法</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">torch.optim.Adagrad()</span><span style=\"color: #6A9955\">#自适应学习率优化算法</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">torch.optim.Adadelta()</span><span style=\"color: #6A9955\">#改进版自适应学习率优化算法</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">torch.optim.RMSprop()</span><span style=\"color: #6A9955\">#均方根反向传播算法</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">torch.optim.Adam()</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">torch.optim.Adamax()</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">torch.optim.SparseAdam()</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">torch.optim.LBFGS()</span><span style=\"color: #6A9955\">#拟牛顿法</span></span></code></pre></div><h3 id=\"正确率评估函数-可选\"><a href=\"#正确率评估函数-可选\" class=\"headerlink\" title=\"正确率评估函数(可选)\"></a>正确率评估函数(可选)</h3><p>这个非必须，主要是给人看模型训练的正确率的，一般都要有<br>例如这个：</p>\n<div class=\"language-python\"><button title=\"Copy code\" class=\"copy\"></button><span class=\"lang\">python</span><pre class=\"shiki dark-plus\" style=\"background-color: #1a1a1a\" tabindex=\"0\"><code><span class=\"line\"><span style=\"color: #569CD6\">def</span><span style=\"color: #D4D4D4\"> </span><span style=\"color: #DCDCAA\">evaluate_accuracy</span><span style=\"color: #D4D4D4\">(</span><span style=\"color: #9CDCFE\">data_iter</span><span style=\"color: #D4D4D4\">, </span><span style=\"color: #9CDCFE\">net</span><span style=\"color: #D4D4D4\">):</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">    acc_sum, n = </span><span style=\"color: #B5CEA8\">0.0</span><span style=\"color: #D4D4D4\">, </span><span style=\"color: #B5CEA8\">0</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">    </span><span style=\"color: #C586C0\">for</span><span style=\"color: #D4D4D4\"> X, y </span><span style=\"color: #C586C0\">in</span><span style=\"color: #D4D4D4\"> data_iter:</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">        acc_sum += (net(X).argmax(</span><span style=\"color: #9CDCFE\">dim</span><span style=\"color: #D4D4D4\">=</span><span style=\"color: #B5CEA8\">1</span><span style=\"color: #D4D4D4\">) == y).float().sum().item()</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">        n += y.shape[</span><span style=\"color: #B5CEA8\">0</span><span style=\"color: #D4D4D4\">]</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">    </span><span style=\"color: #C586C0\">return</span><span style=\"color: #D4D4D4\"> acc_sum / n</span></span></code></pre></div><h3 id=\"训练方法\"><a href=\"#训练方法\" class=\"headerlink\" title=\"训练方法\"></a>训练方法</h3><p>告诉python怎么使用数据和模型进行计算</p>\n<div class=\"language-python\"><button title=\"Copy code\" class=\"copy\"></button><span class=\"lang\">python</span><pre class=\"shiki dark-plus\" style=\"background-color: #1a1a1a\" tabindex=\"0\"><code><span class=\"line\"><span style=\"color: #569CD6\">def</span><span style=\"color: #D4D4D4\"> </span><span style=\"color: #DCDCAA\">train_ch3</span><span style=\"color: #D4D4D4\">(</span><span style=\"color: #9CDCFE\">net</span><span style=\"color: #D4D4D4\">, </span><span style=\"color: #9CDCFE\">train_iter</span><span style=\"color: #D4D4D4\">, </span><span style=\"color: #9CDCFE\">test_iter</span><span style=\"color: #D4D4D4\">, </span><span style=\"color: #9CDCFE\">loss</span><span style=\"color: #D4D4D4\">, </span><span style=\"color: #9CDCFE\">num_epochs</span><span style=\"color: #D4D4D4\">, </span><span style=\"color: #9CDCFE\">batch_size</span><span style=\"color: #D4D4D4\">,</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">              </span><span style=\"color: #9CDCFE\">params</span><span style=\"color: #D4D4D4\">=</span><span style=\"color: #569CD6\">None</span><span style=\"color: #D4D4D4\">, </span><span style=\"color: #9CDCFE\">lr</span><span style=\"color: #D4D4D4\">=</span><span style=\"color: #569CD6\">None</span><span style=\"color: #D4D4D4\">, </span><span style=\"color: #9CDCFE\">optimizer</span><span style=\"color: #D4D4D4\">=</span><span style=\"color: #569CD6\">None</span><span style=\"color: #D4D4D4\">):</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">    </span><span style=\"color: #C586C0\">for</span><span style=\"color: #D4D4D4\"> epoch </span><span style=\"color: #C586C0\">in</span><span style=\"color: #D4D4D4\"> </span><span style=\"color: #DCDCAA\">range</span><span style=\"color: #D4D4D4\">(num_epochs):</span><span style=\"color: #6A9955\">#首先对每个周期来说，要执行以下操作：</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">        train_l_sum, train_acc_sum, n = </span><span style=\"color: #B5CEA8\">0.0</span><span style=\"color: #D4D4D4\">, </span><span style=\"color: #B5CEA8\">0.0</span><span style=\"color: #D4D4D4\">, </span><span style=\"color: #B5CEA8\">0</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">        </span><span style=\"color: #C586C0\">for</span><span style=\"color: #D4D4D4\"> X, y </span><span style=\"color: #C586C0\">in</span><span style=\"color: #D4D4D4\"> train_iter:</span><span style=\"color: #6A9955\">#对每个样本</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">            y_hat = net(X)</span><span style=\"color: #6A9955\">#根据模型向前传播算出对应的标签值</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">            l = loss(y_hat, y).sum()</span><span style=\"color: #6A9955\">#计算该样本的损失并使用sum方法并加起来</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">            </span><span style=\"color: #6A9955\"># 对优化器记得梯度清零</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">            optimizer.zero_grad()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">            l.backward() </span><span style=\"color: #6A9955\">#使用backward对损失函数进行反向传播</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">            optimizer.step()  </span><span style=\"color: #6A9955\">#使用SGD算法进行优化</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">            </span><span style=\"color: #6A9955\">#统计正确数和总数(非必须)</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">            train_l_sum += l.item()</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">            train_acc_sum += (y_hat.argmax(</span><span style=\"color: #9CDCFE\">dim</span><span style=\"color: #D4D4D4\">=</span><span style=\"color: #B5CEA8\">1</span><span style=\"color: #D4D4D4\">) == y).sum().item()</span><span style=\"color: #6A9955\">#计算当前的正确率</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">            n += y.shape[</span><span style=\"color: #B5CEA8\">0</span><span style=\"color: #D4D4D4\">]</span><span style=\"color: #6A9955\">#已训练的样本数量要加上当前批次的训练数量</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">        </span><span style=\"color: #6A9955\">#计算正确率(非必须)</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">        test_acc = evaluate_accuracy(test_iter, net)</span><span style=\"color: #6A9955\">#使用评估函数对随机样本进行评估</span></span>\n<span class=\"line\"><span style=\"color: #D4D4D4\">        </span><span style=\"color: #DCDCAA\">print</span><span style=\"color: #D4D4D4\">(</span><span style=\"color: #CE9178\">'epoch </span><span style=\"color: #569CD6\">%d</span><span style=\"color: #CE9178\">, loss </span><span style=\"color: #569CD6\">%.4f</span><span style=\"color: #CE9178\">, train acc </span><span style=\"color: #569CD6\">%.3f</span><span style=\"color: #CE9178\">, test acc </span><span style=\"color: #569CD6\">%.3f</span><span style=\"color: #CE9178\">'</span><span style=\"color: #D4D4D4\"> % (epoch + </span><span style=\"color: #B5CEA8\">1</span><span style=\"color: #D4D4D4\">, train_l_sum / n, train_acc_sum / n, test_acc))</span><span style=\"color: #6A9955\">#输出</span></span></code></pre></div>","feature":true,"text":"兄弟们，看了二十来天的人工智能概述还有实现方法，我终于搞懂了。 一个有关人工智能的pytorch程序应该包含数据集废话，没数据怎么玩啊，这里以FashionMN...","permalink":"/post/人工智能学习笔记","photos":[],"count_time":{"symbolsCount":"6.5k","symbolsTime":"6 mins."},"categories":[{"name":"人工智能","slug":"人工智能","count":4,"path":"api/categories/人工智能.json"}],"tags":[{"name":"Pytorch","slug":"Pytorch","count":1,"path":"api/tags/Pytorch.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E4%B8%80%E4%B8%AA%E6%9C%89%E5%85%B3%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%9A%84pytorch%E7%A8%8B%E5%BA%8F%E5%BA%94%E8%AF%A5%E5%8C%85%E5%90%AB\"><span class=\"toc-text\">一个有关人工智能的pytorch程序应该包含</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E6%95%B0%E6%8D%AE%E9%9B%86\"><span class=\"toc-text\">数据集</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E6%A8%A1%E5%9E%8B\"><span class=\"toc-text\">模型</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E8%87%AA%E5%AE%9A%E4%B9%89%E4%B8%8D%E5%B8%A6%E5%8F%82%E6%95%B0%E7%9A%84%E5%B1%82%EF%BC%9A\"><span class=\"toc-text\">自定义不带参数的层：</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E8%87%AA%E5%AE%9A%E4%B9%89%E5%B8%A6%E5%8F%82%E6%95%B0%E7%9A%84%E5%B1%82%EF%BC%9A\"><span class=\"toc-text\">自定义带参数的层：</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E5%AE%9A%E4%B9%89%E6%A8%A1%E5%9E%8B%E5%B9%B6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%AE%9E%E4%BE%8B\"><span class=\"toc-text\">定义模型并生成模型实例</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E8%AE%BF%E9%97%AE%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E5%8F%82%E6%95%B0%EF%BC%9A\"><span class=\"toc-text\">访问模型中的参数：</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E5%88%9D%E5%A7%8B%E5%8C%96%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E5%8F%82%E6%95%B0%EF%BC%9A\"><span class=\"toc-text\">初始化模型中的参数：</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E8%AF%BB%E5%86%99%E5%92%8C%E4%BF%9D%E5%AD%98\"><span class=\"toc-text\">读写和保存</span></a></li></ol></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0\"><span class=\"toc-text\">损失函数</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E4%BC%98%E5%8C%96%E5%87%BD%E6%95%B0\"><span class=\"toc-text\">优化函数</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E6%AD%A3%E7%A1%AE%E7%8E%87%E8%AF%84%E4%BC%B0%E5%87%BD%E6%95%B0-%E5%8F%AF%E9%80%89\"><span class=\"toc-text\">正确率评估函数(可选)</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95\"><span class=\"toc-text\">训练方法</span></a></li></ol></li></ol>","author":{"name":"Ac-Accelerator","slug":"blog-author","avatar":"/img/avatar.jpg","link":"/","description":"一位计算机技术爱好者，大学在读，目前正在学习C++、Python、Java和计算机系统以及计算机视觉深度学习等相关知识，欢迎交流","socials":{"github":"https://github.com/Ac-Accelerator","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"https://blog.csdn.net/qq_31702515?spm=1000.2115.3001.5343","juejin":"","customs":{"qq":{"icon":"/svg/QQ.svg","link":"http://tool.gljlw.com/qq/?qq=2583832841"}}}},"mapped":true,"hidden":false,"prev_post":{"title":"CSS网页标准布局和美化","uid":"2cd531c7170c2dbe47cea834debb4679","slug":"CSS网页标准布局和美化","date":"2022-04-04T02:57:00.000Z","updated":"2023-03-31T12:26:58.497Z","comments":true,"path":"api/articles/CSS网页标准布局和美化.json","keywords":"C++,Pyhton,Java,算法","cover":"img/css.jpeg","text":"&gt;网页布局就是利用CSS摆盒子。 布局盒子边框border：&lt;line-width&gt; || &lt;line-style&gt; || &lt...","permalink":"/post/CSS网页标准布局和美化","photos":[],"count_time":{"symbolsCount":"1.8k","symbolsTime":"2 mins."},"categories":[{"name":"前端","slug":"前端","count":10,"path":"api/categories/前端.json"}],"tags":[{"name":"CSS","slug":"CSS","count":8,"path":"api/tags/CSS.json"}],"author":{"name":"Ac-Accelerator","slug":"blog-author","avatar":"/img/avatar.jpg","link":"/","description":"一位计算机技术爱好者，大学在读，目前正在学习C++、Python、Java和计算机系统以及计算机视觉深度学习等相关知识，欢迎交流","socials":{"github":"https://github.com/Ac-Accelerator","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"https://blog.csdn.net/qq_31702515?spm=1000.2115.3001.5343","juejin":"","customs":{"qq":{"icon":"/svg/QQ.svg","link":"http://tool.gljlw.com/qq/?qq=2583832841"}}}},"feature":true},"next_post":{"title":"CSS盒子和背景","uid":"e8e899a32442557ba38f16b4d4d99793","slug":"CSS盒子和背景","date":"2022-04-03T02:57:00.000Z","updated":"2023-03-31T12:25:44.770Z","comments":true,"path":"api/articles/CSS盒子和背景.json","keywords":"C++,Pyhton,Java,算法","cover":"img/css.jpeg","text":"Emmet语法快速开发对于HTML标签: 1．生成标签直接输入标签名按tab键即可比如div然后tab键，就可以生成&lt;div&gt;&lt;/div&gt...","permalink":"/post/CSS盒子和背景","photos":[],"count_time":{"symbolsCount":"1.4k","symbolsTime":"1 mins."},"categories":[{"name":"前端","slug":"前端","count":10,"path":"api/categories/前端.json"}],"tags":[{"name":"CSS","slug":"CSS","count":8,"path":"api/tags/CSS.json"}],"author":{"name":"Ac-Accelerator","slug":"blog-author","avatar":"/img/avatar.jpg","link":"/","description":"一位计算机技术爱好者，大学在读，目前正在学习C++、Python、Java和计算机系统以及计算机视觉深度学习等相关知识，欢迎交流","socials":{"github":"https://github.com/Ac-Accelerator","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"https://blog.csdn.net/qq_31702515?spm=1000.2115.3001.5343","juejin":"","customs":{"qq":{"icon":"/svg/QQ.svg","link":"http://tool.gljlw.com/qq/?qq=2583832841"}}}},"feature":true}}